<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>FactualMxValidation.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">CIA</a> &gt; <a href="index.source.html" class="el_package">com.mbww.cia.qa.denormalizer</a> &gt; <span class="el_source">FactualMxValidation.scala</span></div><h1>FactualMxValidation.scala</h1><pre class="source lang-java linenums">package com.mbww.cia.qa.denormalizer

import com.mbww.cia.common.Module
import org.json.JSONObject
import org.slf4j.LoggerFactory
import com.mbww.cia.common.Spark._
import org.apache.spark.sql.functions._
import org.joda.time.DateTime


<span class="fc" id="L11">class FactualMxValidation  extends  Module{</span>
<span class="fc" id="L12">  val log = LoggerFactory.getLogger(&quot;CIA Pipeline&quot;)</span>

  def validation(factualCwParquetPath: String,factualFactParquetBasePath: String, factualt_place_category_labels: String, infobasePath: String, outputPath: String, startDate: String, endDate: String): Boolean=
  {

<span class="fc" id="L17">    val factual_crosswalk_data = spark.read.load(factualCwParquetPath)</span>
<span class="fc" id="L18">    factual_crosswalk_data.createOrReplaceTempView(&quot;factual_crosswalk_data&quot;)</span>
<span class="fc" id="L19">    val factual_dimension_data = spark.read.load(factualt_place_category_labels)</span>
<span class="fc" id="L20">    factual_dimension_data.createOrReplaceTempView(&quot;factual_dimension_data&quot;)</span>
<span class="fc" id="L21">    val uniquekeys = spark.read.load(infobasePath)</span>
<span class="fc" id="L22">    uniquekeys.createOrReplaceTempView(&quot;uniquekeys&quot;)</span>

<span class="fc" id="L24">    val startDay=new DateTime(s&quot;${startDate}&quot;).getDayOfMonth()</span>
<span class="fc" id="L25">    println(s&quot;startDay&quot; + startDay)</span>
<span class="fc" id="L26">    val monthProcess=new DateTime(s&quot;${startDate}&quot;).getMonthOfYear()</span>
<span class="pc bpc" id="L27" title="1 of 2 branches missed.">    val processingMonth=(if (monthProcess &lt; 10) &quot;0&quot; + monthProcess else &quot;&quot; + monthProcess).toString</span>
<span class="fc" id="L28">    println(s&quot;processedMonth&quot; + processingMonth)</span>
<span class="fc" id="L29">    val processingYear=new DateTime(s&quot;${startDate}&quot;).getYear.toString</span>
<span class="fc" id="L30">    println(s&quot;processingYear&quot; + processingYear)</span>
<span class="fc" id="L31">    val endDay=new DateTime(s&quot;${endDate}&quot;).getDayOfMonth()</span>
<span class="fc" id="L32">    println(s&quot;endDay&quot; + endDay)</span>


<span class="fc" id="L35">    val factual_per_month_partitioned_data = spark.read.load(factualFactParquetBasePath + &quot;year=&quot; + processingYear + &quot;/month=&quot; + processingMonth + &quot;/&quot;)</span>
<span class="fc" id="L36">    factual_per_month_partitioned_data.createOrReplaceTempView(&quot;factual_per_month_partitioned_data&quot;)</span>
    /** join fact and dimension*/
<span class="fc" id="L38">    val factual_per_month_partitioned_data_category = spark.sql(&quot;SELECT a.factual_id, a.local_timestamp, a.locality, a.region,postcode, a.dwell_minutes , a.place_id, a.place_name, a.place_chain_id, b.tier1 FROM factual_per_month_partitioned_data a inner join factual_dimension_data b ON a.place_id = b.place_id&quot;)</span>
<span class="fc" id="L39">    factual_per_month_partitioned_data_category.createOrReplaceTempView(&quot;factual_per_month_partitioned_data_category&quot;)</span>
    /** create myTimestamp*/
<span class="fc" id="L41">    val factual_per_month_partitioned_data_timestamp = spark.sql(&quot;SELECT local_timestamp,  to_timestamp(REPLACE(SUBSTRING(local_timestamp,1,19),'T',' '),'yyyy-MM-dd HH:mm:ss') as myTimestamp, factual_id, locality, region, postcode, dwell_minutes, place_id, place_name, place_chain_id, tier1  FROM factual_per_month_partitioned_data_category&quot;)</span>
<span class="fc" id="L42">    factual_per_month_partitioned_data_timestamp.createOrReplaceTempView(&quot;factual_per_month_partitioned_data_timestamp&quot;)</span>
    /** join cw and infobase*/
<span class="fc" id="L44">    val factual_cw_uniqueKeys = spark.sql(&quot;SELECT a.fc_id, a.consumer_pel,  b.numericId, b.stringId FROM factual_crosswalk_data a inner join uniquekeys b ON a.consumer_pel = b.stringId&quot;)</span>
<span class="fc" id="L45">    factual_cw_uniqueKeys.createOrReplaceTempView(&quot;factual_cw_uniqueKeys&quot;)</span>
    /** fact without duplicates*/
<span class="fc" id="L47">    val factual_filter_dup = spark.sql(&quot;select * from (select ROW_NUMBER()  OVER(PARTITION BY factual_id, local_timestamp, myTimestamp, locality, region, postcode, dwell_minutes, place_id, place_name, place_chain_id, tier1  ORDER BY local_timestamp) as RowNum,  factual_id, local_timestamp,myTimestamp, locality, region, postcode, dwell_minutes, place_id, place_name, place_chain_id, tier1  from factual_per_month_partitioned_data_timestamp)x where RowNum=1&quot;)</span>
<span class="fc" id="L48">    factual_filter_dup.createOrReplaceTempView(&quot;factual_filter_dup&quot;)</span>

    /** join cw infobase and fact*/
<span class="fc" id="L51">    val factual_fact_joined_with_crosswalk_unique_keys = spark.sql(&quot;SELECT a.local_timestamp,  a.myTimestamp, a.factual_id, a.locality, a.region, a.postcode, a.dwell_minutes, a.place_id, a.place_name, a.place_chain_id, a.tier1,b.numericId, b.stringId, b.fc_id, b.consumer_pel   FROM factual_filter_dup a inner join factual_cw_uniqueKeys b ON (a.factual_id=b.fc_id )&quot;)</span>
<span class="fc" id="L52">    factual_fact_joined_with_crosswalk_unique_keys.createOrReplaceTempView(&quot;factual_fact_joined_with_crosswalk_unique_keys&quot;)</span>
    /** include date*/
<span class="fc" id="L54">    val factual_facts_with_date = spark.sql(&quot;SELECT local_timestamp, TO_DATE(CAST(UNIX_TIMESTAMP(myTimestamp,'yyyy-MM-dd') AS TIMESTAMP)) AS date,&quot; +</span>
      &quot; hour(myTimestamp) as hour, factual_id, locality, region, postcode, dwell_minutes, place_id, place_name, place_chain_id, tier1,fc_id,consumer_pel,numericId,stringId FROM factual_fact_joined_with_crosswalk_unique_keys&quot;)
<span class="fc" id="L56">    factual_facts_with_date.createOrReplaceTempView(&quot;factual_facts_with_date&quot;)</span>
<span class="fc" id="L57">    spark.sql(&quot;SELECT date, local_timestamp, factual_id, locality, region, postcode, dwell_minutes, place_id, place_name, place_chain_id, tier1,&quot; +</span>
<span class="fc" id="L58">      &quot; fc_id, numericId, stringId,  case when hour=0 then '23:00-01:00' else case when hour=1 then '23:00-01:00' else case when hour=2 then '02:00-04:00' else case when hour=3 then '02:00-04:00' else case when hour=4 then '02:00-04:00' else case when hour=5 then '05:00-06:00' else case when hour=6 then '05:00-06:00' else case when hour=7 then '07:00-08:00' else case when hour=8 then '07:00-08:00' else case when hour=9 then '09:00-10:00' else case when hour=10 then '09:00-10:00' else case when hour=11 then '11:00-13:00' else case when hour=12 then '11:00-13:00' else case when hour=13 then '11:00-13:00' else case when hour=14 then '14:00-16:00' else case when hour=15 then '14:00-16:00' else case when hour=16 then '14:00-16:00' else case when hour=17 then '17:00-18:00' else case when hour=18 then '17:00-18:00' else case when hour=19 then '19:00-20:00' else case when hour=20 then '19:00-20:00' else case when hour=21 then '21:00-22:00' else case when hour=22 then '21:00-22:00' else case when hour=23 then '23:00-01:00' else case when hour=24 then '23:00-01:00'  else 'NA' END END END END END END END END END END END END END END END END END END END END END END END END END as daypart FROM factual_facts_with_date&quot;).createOrReplaceTempView(&quot;factual_data_with_dayparts&quot;)</span>
<span class="fc" id="L59">    val dfResult = spark.sql(&quot;SELECT * FROM factual_data_with_dayparts&quot;)</span>
<span class="fc" id="L60">    val selectColumns = Seq(col(&quot;numericId&quot;).as(&quot;mbid&quot;),col(&quot;stringId&quot;).as(&quot;mbid_value&quot;),col(&quot;factual_id&quot;), col(&quot;local_timestamp&quot;)</span>
<span class="fc" id="L61">      ,  col(&quot;date&quot;),col(&quot;locality&quot;),</span>
<span class="fc" id="L62">      col(&quot;region&quot;), col(&quot;postcode&quot;), col(&quot;dwell_minutes&quot;), col(&quot;place_id&quot;),</span>
<span class="fc" id="L63">      col(&quot;place_name&quot;), col(&quot;place_chain_id&quot;),</span>
<span class="fc" id="L64">      col(&quot;daypart&quot;), col(&quot;tier1&quot;).as(&quot;category&quot;))</span>

<span class="fc" id="L66">    val expectedResult=dfResult.select(selectColumns: _*).filter(&quot;locality is not null&quot;).filter(&quot;region is not null&quot;)</span>

    //val maxDay = UtilsFunction.getDaysInMonth(processingYear,processingMonth)

<span class="pc bpc" id="L70" title="1 of 2 branches missed.">    for (day &lt;- startDay to endDay)</span>
    {
<span class="pc bpc" id="L72" title="1 of 2 branches missed.">      val newDay = (if (day &lt; 10) &quot;0&quot; + day else &quot;&quot; + day);</span>
<span class="fc" id="L73">      println(s&quot;newDay&quot; + newDay)</span>
<span class="fc" id="L74">      val filterdDf=expectedResult.filter(col(&quot;date&quot;)===processingYear +&quot;-&quot;+ processingMonth +&quot;-&quot;+ newDay)</span>
<span class="fc" id="L75">      val expectedCount=filterdDf.count</span>
<span class="fc" id="L76">      val actualData=spark.read.load(outputPath+&quot;/date=&quot;+ processingYear +&quot;-&quot;+ processingMonth +&quot;-&quot;+ newDay +&quot;/&quot;)</span>
<span class="fc" id="L77">      val localityDf=actualData.filter(&quot;locality is null&quot;)</span>
<span class="fc" id="L78">      val localityNullCount=localityDf.count</span>

<span class="fc" id="L80">      val regionDf=actualData.filter(&quot;region is null&quot;)</span>
<span class="fc" id="L81">      val regionNullCount=regionDf.count</span>


<span class="pc bpc" id="L84" title="2 of 4 branches missed.">      if(!((localityNullCount==0) &amp;&amp; (regionNullCount==0)))</span>
      {
<span class="nc" id="L86">        throw new RuntimeException(</span>
<span class="nc" id="L87">          s&quot;localityNullCount is :  ${localityNullCount}  and  regionNullCount is : ${regionNullCount}&quot;)</span>
      }

<span class="fc" id="L90">      val actualCount=actualData.count()</span>
<span class="pc bpc" id="L91" title="1 of 2 branches missed.">      if(!(actualCount==expectedCount))</span>
      {
<span class="nc" id="L93">        throw new RuntimeException(</span>
<span class="nc" id="L94">          s&quot;Count of actual custom-attribute-store:  ${actualCount} is not matching with expected custom-attribute-store count ${expectedCount}&quot;)</span>

      }
<span class="fc" id="L97">      log.info(s&quot;Count of actual custom-attribute-store:  ${actualCount} is matching with expected custom-attribute-store count ${expectedCount}&quot;)</span>
    }
<span class="fc" id="L99">    true</span>
  }

  override def run(args: JSONObject): Unit = {
<span class="fc" id="L103">    val factualCwParquetPath = args.getString(&quot;factualCwParquetPath&quot;)</span>
<span class="fc" id="L104">    val factualFactParquetBasePath = args.getString(&quot;factualFactParquetBasePath&quot;)</span>
<span class="fc" id="L105">    val factualt_place_category_labels = args.getString(&quot;factualt_place_category_labels&quot;)</span>
<span class="fc" id="L106">    val infobasePath = args.getString(&quot;infobasePath&quot;)</span>
<span class="fc" id="L107">    val outputPath = args.getString(&quot;outputPath&quot;)</span>
<span class="fc" id="L108">    val endDate=args.getString(&quot;endDate&quot;)</span>
<span class="fc" id="L109">    val startDate=args.getString(&quot;startDate&quot;)</span>

<span class="fc" id="L111">    validation(</span>
<span class="fc" id="L112">      factualCwParquetPath,</span>
<span class="fc" id="L113">      factualFactParquetBasePath,</span>
<span class="fc" id="L114">      factualt_place_category_labels,</span>
<span class="fc" id="L115">      infobasePath,</span>
<span class="fc" id="L116">      outputPath,</span>
<span class="fc" id="L117">      startDate,</span>
<span class="fc" id="L118">      endDate)</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.1.201803210924</span></div></body></html>