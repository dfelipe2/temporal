<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>AcxiomValidation.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">CIA</a> &gt; <a href="index.source.html" class="el_package">com.mbww.cia.qa.denormalizer</a> &gt; <span class="el_source">AcxiomValidation.scala</span></div><h1>AcxiomValidation.scala</h1><pre class="source lang-java linenums">package com.mbww.cia.qa.denormalizer

import com.mbww.cia.common.Spark._
import com.mbww.cia.common.{Module, _}
import com.mbww.cia.qa.utils.ValidationUtils
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions.lit
import org.apache.spark.storage.StorageLevel
import org.json.JSONObject
import org.slf4j.LoggerFactory
import scala.util.Random
import scala.collection.parallel.ForkJoinTaskSupport
import scala.concurrent.forkjoin

<span class="fc" id="L15">class AcxiomValidation extends Module  {</span>
<span class="fc" id="L16">  val log = LoggerFactory.getLogger(&quot;CIA Pipeline&quot;)</span>
  def denormalizeValidationWithoutPivoting(parquetAcxiomDF: DataFrame,
                                 output_columns: Seq[String],
                                 filter_expr: String,
                                 attribute_id: String,
                                 drop_columns: Seq[String],
                                 output_path: String,
<span class="nc" id="L23">                                 thread_count: Int = 1): Unit = {</span>
<span class="fc" id="L24">    val columns = parquetAcxiomDF.drop(drop_columns: _*).columns</span>
<span class="fc" id="L25">    val pColumns =  Random.shuffle(columns.toList).take(10).par</span>
<span class="fc" id="L26">    pColumns.tasksupport = new ForkJoinTaskSupport(new forkjoin.ForkJoinPool(thread_count))</span>
<span class="pc bpc" id="L27" title="1 of 2 branches missed.">    pColumns.foreach(c =&gt; {</span>
<span class="fc" id="L28">      log.info(s&quot;Testing attribute name is $attribute_id=$c&quot;)</span>
<span class="fc" id="L29">      val output_attribute = concatToPath(output_path, s&quot;$attribute_id=$c&quot;)</span>
<span class="fc" id="L30">      val actualDF=parquetAcxiomDF.filter(parquetAcxiomDF(c) === &quot;1&quot;)</span>
<span class="fc" id="L31">      val expectedDF =spark.read.parquet(output_attribute)</span>
<span class="fc" id="L32">      ValidationUtils.validateCount(actualDF, expectedDF)</span>
    })
  }

  def denormalizeWithPivoting(parquetAcxiomDF: DataFrame,
                                           output_columns: Seq[String],
                                           filter_expr: String,
                                           attribute_id: String,
                                           drop_columns: Seq[String],
                                           output_path: String,
<span class="nc" id="L42">                                           thread_count: Int = 1): Unit = {</span>

<span class="fc" id="L44">    val columns = parquetAcxiomDF.drop(drop_columns: _*).columns</span>
<span class="fc" id="L45">    Random.shuffle(columns.toList).take(10).</span>
<span class="pc bpc" id="L46" title="1 of 2 branches missed.">      foreach(c =&gt; {</span>
<span class="fc" id="L47">      val values = parquetAcxiomDF</span>
<span class="fc" id="L48">        .select(c)</span>
<span class="fc" id="L49">        .groupBy(c)</span>
<span class="fc" id="L50">        .agg(lit(1))</span>
<span class="fc" id="L51">        .select(c)</span>
<span class="fc" id="L52">        .collect</span>
<span class="fc" id="L53">        .map(r =&gt; r.getString(0))</span>
<span class="fc" id="L54">      val pValues =  Random.shuffle(values.toList).take(10).par</span>
<span class="fc" id="L55">      pValues.tasksupport = new ForkJoinTaskSupport(new forkjoin.ForkJoinPool(thread_count))</span>
<span class="pc bpc" id="L56" title="1 of 2 branches missed.">      pValues.foreach(v =&gt; {</span>
<span class="fc" id="L57">        val attribute_name = s&quot;${c}_${v}&quot;</span>
<span class="fc" id="L58">        log.info(&quot;Testing attribute name is &quot; + attribute_name)</span>
<span class="fc" id="L59">        val output_attribute = concatToPath(output_path, s&quot;$attribute_id=$attribute_name&quot;)</span>
<span class="fc" id="L60">        var actualDF=parquetAcxiomDF.filter(parquetAcxiomDF(c) === v)</span>
<span class="fc" id="L61">        val expectedDF =spark.read.parquet(output_attribute)</span>
<span class="fc" id="L62">        ValidationUtils.validateCount(actualDF, expectedDF)</span>
      })
    })
  }

  def denormalizeValidation(acxiom_path: String,
                            output_columns: Seq[String],
                            filter_expr: String,
                            attribute_id: String,
                            drop_columns: Seq[String],
                            pivot_values: Boolean,
                            output_path: String,
<span class="pc" id="L74">                            thread_count: Int = 1):Unit = {</span>
<span class="fc bfc" id="L75" title="All 2 branches covered.">    val parquetAcxiomDF = if (!filter_expr.isEmpty) {</span>
<span class="fc" id="L76">      spark.read.parquet(acxiom_path).filter(filter_expr).persist(StorageLevel.MEMORY_AND_DISK)</span>
    } else {
<span class="fc" id="L78">      spark.read.load(acxiom_path)</span>
    }
<span class="fc bfc" id="L80" title="All 2 branches covered.">    if (pivot_values) {</span>
<span class="fc" id="L81">      denormalizeWithPivoting(parquetAcxiomDF,</span>
<span class="fc" id="L82">        output_columns,</span>
<span class="fc" id="L83">        filter_expr,</span>
<span class="fc" id="L84">        attribute_id,</span>
<span class="fc" id="L85">        drop_columns,</span>
<span class="fc" id="L86">        output_path,</span>
<span class="fc" id="L87">        thread_count)</span>

    } else {
<span class="fc" id="L90">      denormalizeValidationWithoutPivoting(parquetAcxiomDF,</span>
<span class="fc" id="L91">        output_columns,</span>
<span class="fc" id="L92">        filter_expr,</span>
<span class="fc" id="L93">        attribute_id,</span>
<span class="fc" id="L94">        drop_columns,</span>
<span class="fc" id="L95">        output_path,</span>
<span class="fc" id="L96">        thread_count)</span>
    }

  }


  def validation(acxiom_paths: Seq[String],
               output_columns: Seq[String],
               filter_expr: String,
               attribute_id: String,
               pivot_values: Boolean,
               drop_columns: Seq[String],
               output_path: String,
<span class="nc" id="L109">               thread_count: Int = 1): Unit = {</span>

<span class="pc bpc" id="L111" title="1 of 2 branches missed.">    acxiom_paths.foreach(acxiom_path =&gt; {</span>
<span class="fc" id="L112">      denormalizeValidation(</span>
<span class="fc" id="L113">        acxiom_path,</span>
<span class="fc" id="L114">        output_columns,</span>
<span class="fc" id="L115">        filter_expr,</span>
<span class="fc" id="L116">        attribute_id,</span>
<span class="fc" id="L117">        drop_columns,</span>
<span class="fc" id="L118">        pivot_values,</span>
<span class="fc" id="L119">        output_path,</span>
<span class="fc" id="L120">        thread_count)</span>
     })
  }

  override def run(args: JSONObject): Unit = {

<span class="fc" id="L126">    val acxiom_paths = args.getSeq[String](&quot;acxiom_paths&quot;)</span>
<span class="fc" id="L127">    val output_columns = args.getSeq[String](&quot;output_columns&quot;)</span>
<span class="fc" id="L128">    val output_path = args.getString(&quot;output_path&quot;)</span>
<span class="fc" id="L129">    val acxiom_filter_expr = args.getString(&quot;acxiom_filter_expr&quot;)</span>
<span class="fc" id="L130">    val partition_attribute_id = args.getString(&quot;partition_attribute_id&quot;)</span>
<span class="fc" id="L131">    val pivot_values = args.getBoolean(&quot;pivot_values&quot;)</span>
<span class="fc" id="L132">    val drop_columns = args.getSeq[String](&quot;drop_columns&quot;)</span>
<span class="fc" id="L133">    val thread_count = args.getOrElse[Int](&quot;thread_count&quot;, 1)</span>

<span class="fc" id="L135">    validation(</span>
<span class="fc" id="L136">      acxiom_paths,</span>
<span class="fc" id="L137">      output_columns,</span>
<span class="fc" id="L138">      acxiom_filter_expr,</span>
<span class="fc" id="L139">      partition_attribute_id,</span>
<span class="fc" id="L140">      pivot_values,</span>
<span class="fc" id="L141">      drop_columns,</span>
<span class="fc" id="L142">      output_path,</span>
<span class="fc" id="L143">      thread_count)</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.1.201803210924</span></div></body></html>